arXiv:2307.12336v1 [cs.LG] 23 Jul 2023

TabADM: Unsupervised Tabular Anomaly Detection
with Diffusion Models
Guy Zamberg1 , Moshe Salhov2,4 , Ofir Lindenbaum3 , Amir Averbuch2
1
School of Electrical Engineering, Tel Aviv University, Israel
2
School of Computer Science, Tel Aviv University, Israel
3
Faculty of Engineering, Bar-Ilan University, Israel
4
Playtika LTD, St. Herzllya, Israel

Abstract
Tables are an abundant form of data with use cases across all scientific fields. Realworld datasets often contain anomalous samples that can negatively affect downstream analysis. In this work, we only assume access to contaminated data and
present a diffusion-based probabilistic model effective for unsupervised anomaly
detection. Our model is trained to learn the density of normal samples by utilizing
a unique rejection scheme to attenuate the influence of anomalies on the density
estimation. At inference, we identify anomalies as samples in low-density regions.
We use real data to demonstrate that our method improves detection capabilities
over baselines. Furthermore, our method is relatively stable to the dimension of
the data and does not require extensive hyperparameter tuning.

1

Introduction

Anomaly detection, also known as outlier detection, involves identifying “abnormal” instances
within datasets. These exceptional instances are called anomalies or outliers, while “normal” instances are known as inliers. In 1969, Grubbs [15] initially defined an outlier as ”one that appears
to deviate markedly from other members of the sample in which it occurs.” Anomaly detection has
numerous applications, such as fraud detection [9, 21], network intrusion detection [24, 39], medical
diagnostics [22, 23], automatic explosion detection [7, 28] and social media [36] to name some.
To address the problem of anomaly detection, various methods have been proposed. The solutions can be classified into three settings: 1) Supervised, which requires a training set with labeled
inliers/outliers but is limited due to the expensive data labeling. 2) Semi-supervised, which only
requires pure single-class training data labeled as inliers without any outliers involved during training. 3) Unsupervised, which deals with completely unlabeled data mixed with outliers and does not
require any data labeling for training. This paper deals with unsupervised anomaly detection, an
approach that is widely applicable in practice due to the prevalence of unlabeled data.
Existing unsupervised anomaly detection methods can be divided into different groups. The first
group is subspace-based methods [17, 27, 40, 48, 49]. The central assumption regarding these methods is that the normal data can be fully embedded in a lower-dimensional subspace. This assumption
is not always valid and may constrain the range of applicable data distributions. Moreover, the performance of these methods depends heavily on the choice of hyperparameters used to define the
subspace.
Another family is based on data proximities or distances. Examples include K-Nearest Neighbors
(KNN) [33], Local Outlier Factor (LOF) [8], and Cluster-Based Local Outlier Factor (CBLOF) [18].
These methods define a data point as an outlier when its locality (or proximity) is sparsely populated.
Proximity-based methods are usually susceptible to the choice of distance measures. They also
under-perform on high-dimensional data, where the curse of dimensionality causes distances to

become less meaningful [1, 2]. In addition, they typically require careful hyperparameter tuning,
such as the number of neighbors or cluster size, which greatly influence their performance.
Lastly, a group of probabilistic methods model the underlying distribution of the normal data and
then identify data points exhibiting low probability under the model as potential anomalies. Particular methods [26, 46] limit the potential distributions by imposing assumptions on the interdependence of features or a specific parametric distribution. Additionally, some methods rely on Variational Autoencoders (VAEs) [3] and Generative Adversarial Networks (GANs) [11, 37]. These
methods may suffer from mode collapse, and hyperparameter tuning strongly influences their performance.
To overcome the above limitations, such as the reliance on prior assumptions that may restrict the
generality of the data distribution, the challenging task of hyperparameter tuning, and the difficulty
of coping with the curse of dimensionality in high-dimensional data, we introduce a novel approach
from the probabilistic models family called Unsupervised Tabular Anomaly Detection with Diffusion Models (TabADM). On a high level, TabADM estimates the data distribution using a robust
diffusion generative model and then assigns an anomaly score to a new sample in correspondence
to its probability of being generated by the model. Specifically, we rely on the training loss term
to construct the anomaly score. To robustify the density estimation, we propose a sample rejection
procedure to attenuate the influence of anomalies during training.
Our contributions are:
• Develop a method based on diffusion models for tabular anomaly detection. This method
utilizes the stability property of diffusion models to avoid the challenge of hyperparameter
tuning. Furthermore, it can be fully executed on a single laptop without requiring a GPU
for most existing datasets.
• Propose an anomaly rejection scheme to improve performance when the training set has
outliers. We verify it on three different datasets and present scores improvement in all of
them.
• Benchmark our method using multiple tabular datasets, demonstrating superior results with
respect to two evaluation metrics compared with eleven popular detectors. In addition, our
model significantly outperforms other competitors on high-dimensional datasets.
In this paper, we first provide a discussion of related work in the field of probabilistic models for
anomaly detection in tabular data (Sec. 2), followed by a description of our problem formulation
and method (Sec. 3). We then detail the experimental setup and report the results (Sec. 4). Finally,
we discuss our findings and suggestions for future research directions (Sec. 5).

2

Related Work

Our method can be categorized under the family of probabilistic anomaly detection schemes. In this
section, we first overview various probabilistic methods. Then, we discuss existing approaches for
anomaly detection with diffusion models.
Parametric and non-parametric probabilistic methods. Probabilistic models are usually categorized into two main groups, parametric and non-parametric. Methods that assume a specific
parametric form of the underlying distribution are known as parametric methods. These methods
aim to learn the parameters through a fitting process. A Common parametric framework is Gaussian
Mixture Models based methods such as [46], in which the underlying distribution is modeled as a
combination of multiple Gaussian distributions, and only the parameters of each Gaussian component are estimated. In contrast, non-parametric methods do not assume any parametric model for
the data. Some “shallow” non-parametric methods include Histogram-Based Outlier Score (HBOS)
[13], which uses a histogram to estimate the underlying distribution of the data, and Empirical Cumulative distribution based Outlier Detection (ECOD) [25], which estimates the density using an
empirical cumulative distribution of each feature independently. Following the revolution of deep
neural networks, “deep” non-parametric methods have been developed. Such as Single-Objective
Generative Adversarial Active Learning (SO-GAAL) [30] that utilizes GANs as the primary generative model and active learning to enhance detection performance. More recently, [34] proposed
2

variance stabilized density estimation for anomaly detection implemented using an autoregressive
model.
Diffusion models for anomaly detection Diffusion models [19] are a class of generative models that are used in many applications such as image generation [12], video generation [20], textto-image generation [4, 35], semantic segmentation [5, 44] and waveform signal processing [10].
Diffusion models have also been utilized in anomaly detection tasks. For instance, some methods
[43, 45] focus on identifying anomalous regions within images, while others like [41] detect anomalous frames in videos. However, to the best of our knowledge, no existing methods for detecting
anomalies in tabular data employ diffusion models.

3

Method

We begin by presenting the problem formulation for unsupervised anomaly detection. Then, we explain our proposed approach with a brief theoretical review of diffusion models. Lastly, we describe
the algorithm and the network architecture.
3.1

Problem Formulation

Setup. We follow the setup given in [31] for the problem of unsupervised anomaly detection
on tabular data. Suppose we have a tabular dataset S ∈ Rn×d consisting of n samples xi
(i = 1, 2, ..., n) with d dimensions. Each sample xi could either be a “normal” sample drawn
from the data density distribution q(x) or an “anomaly” drawn from an unknown corruption process. We also assume that anomaly samples are located in low-density regions. Our goal is to train
an anomaly classifier M on S and then, given a new data sample that is not part of S, we want to
assign an anomaly score indicating the degree to which it is anomalous (higher score means it more
likely to be an anomaly).
Proposed Approach. Following probabilistic anomaly detection approach, we train M on contaminated S to model the density qS (x). Assuming that anomaly samples are located in low-density
regions, we approximate that qS (x) = q(x). However, we take into account that the presence of
anomalies has a detrimental effect on the modeling process. Therefore, we rely on the training loss
to assign an anomaly score for an unseen data sample at inference. As demonstrated in the next
paragraph, the loss is based on the log-likelihood of the model given the training data. Samples
with low probability density under the learned distribution q(x) are more likely to be anomalies and
result in high loss values. Hence it can serve as a quantitative measure of abnormality. Following
the success of diffusion models in generative modeling, we present a diffusion architecture to model
q(x). We now provide a concise overview of the diffusion framework.
Density modeling with diffusion models. We briefly introduce the theory of diffusion models
mentioned in [19]. We begin by defining the data distribution x0 ∼ q(x0 ), where x0 ∈ Rd . We
fix a Markov chain to a noising process in which Gaussian noise is gradually added to x0 through
T consecutive diffusion steps, producing latent variables x1 , ..., xT of noisy samples with the same
dimensionality as x0 . Particularly, for a noising variance schedule β1 , ..., βT :
q (x1:T |x0 ) :=

T
Y

q (xt |xt−1 ) ,

 p

q (xt |xt−1 ) := N xt ; 1 − βt xt−1 , βt I .

t=1

A notable property regarding q(xt |xo ) is that it can be expressed as Gaussian distribution. Let
Qt
αt := 1 − βt and αt := s=1 αs :
√
q(xt |x0 ) = N (xt ; ᾱt x0 , (1 − ᾱt )I),
(1)
Hence:
xt =

√

ᾱt x0 +

√

1 − ᾱt ϵ, ϵ ∼ N (0, I).

(2)

Using Bayes theorem on Eq. 1:
q (xt−1 |xt , x0 ) = N (xt−1 ; µ̃t (xt , x0 ) , β̃t I),
3

(3)

√
αt (1 − ᾱt−1 )
ᾱt−1 βt
1 − ᾱt−1
x0 +
xt , β̃t :=
βt .
1 − ᾱt
1 − ᾱt
1 − ᾱt
We aim to learn the data distribution q(x0 ). We define distribution pθ (x0 ) towards this goal. Since
q(xt |xt−1 ) is Gaussian and if βt is small for all t, then q(xt−1 |xt ) is also Gaussian. Thus we can
approximate q(xt−1 |xt ) using a neural network:
pθ (xt−1 |xt ) = N (xt−1 ; µ̃θ (xt , t), Σθ (xt , t).
(4)
Training the model such that pθ (x0 ) estimates q(x0 ), we optimize variational lower bound on the
log likelihood:
Lvlb := L0 + L1 + ... + LT ,
(5)
L0 := − log pθ (x0 |x1 ),
Lt−1 := DKL (q(xt−1 |xt , x0 )||pθ (xt−1 |xt ),
LT := DKL (q(xt |x0 )||p(xT )).
Ho et al. [19] found out that objective loss (5) can be simplified based on equivalency of (3) and (4)
to the sum of mean squared errors between ϵ and ϵθ (xt , t):
Lsimple (θ) := Et,x0 ,ϵ [||ϵ − ϵθ (xt , t)||22 ].
(6)
More specifically, the model ϵθ (xt , t) is trained to predict the true noise ϵ by minimizing the simplified objective loss (6). Each sample xt is produced using Eq. (2), by randomly drawing x0 , t and
ϵ.
√

where µ̃t (xt , x0 ) :=

3.2

TabADM

The TabADM algorithm is composed of two sequential components, namely train and inference. In
the training phase, the model estimates the data distribution q(x) of the training data. In addition, we
include an anomaly rejection scheme during training to minimize the influence of existing anomalies
in the data. At inference, an anomaly score is assigned for each sample in the test data based on a
summation of loss values at each diffusion timestep. These parts will be described in detail in Sec.
3.2.1 and 3.2.2. We conclude this part by presenting our architecture in Sec. 3.2.3.
3.2.1

Train

Algorithm 1 describes the train part of TabADM algorithm. We train a model ϵθ (xt , t) to estimate
the density q(x) of the training data S ∈ Rn×d . As outlined in section 3.1, the estimation of
q(x) involves the minimization of the objective loss (Eq. 6) through a well-defined procedure.
Specifically, the data is first normalized to be in the [−1, 1] interval, and a loop over e steps is
executed. At each step, a k-sample batch x0 is drawn from S. In addition, a Gaussian noise ϵ and a
timesteps array t with k copies of a randomly picked timestep t are created to generate xt according
to Eq. 2. The model ϵθ (xt , t) estimates the true noise ϵ and the loss (Eq. 6) is calculated.
Anomaly rejection scheme To reduce the impact of potential anomalies S, we utilize the
loss function to estimate the probability that a sample is abnormal. We introduce the function
lastk−m (loss), which sorts the loss values in a batch of k samples in descending order and keeps
only the last k−m values. Stochastic gradient descent (SGD) is applied using the lastk−m (loss) to
conduct the train iteration.
Algorithm 1 Train
Input: train data S ∈ Rn×d , batch size k ∈ N, train steps e ∈ N, rejection samples m ∈ N, diffusion
timesteps T ∈ N
1: Normalize S
2: for i = 1 to e do
3:
Sample x0 ∈ S
▷ x0 ∈ Rk×d
4:
Sample ϵ ∼ Nk×d (0, I)
5:
Sample t ∈ U({1, ..., T })
6:
Create√array t with
√ k copies of t
7:
xt = ᾱt x0 + 1 − ᾱt ϵ
▷ Eq. 2
8:
loss = ||ϵ − ϵθ (xt , t)||22
▷ loss ∈ Rk
9:
SGD(lastk−m (loss))
10: end for

4

3.2.2

Inference

Algorithm 2 describes the inference part of TabADM, which generates anomaly scores O ∈ Rk to
each sample in test data S ∈ Rk×d . To begin, we normalize S to the [−1, 1] interval according to
the train data. In addition, we initialize the output anomaly scores array O with k zeros and generate
a Gaussian noise matrix E ∼ NT ×d (0, I). For each sample in S, a sequence (xt )Tt=1 of noisy data
samples is generated, where each xt is created from timestep t and noise Et (Eq. 2). The total loss
for each sample is computed by summing the loss values across all timesteps, and it is stored in the
corresponding sample entry in O.
Algorithm 2 Inference
Input: test data S ∈ Rk×d , diffusion timesteps T ∈ N
Output: Anomaly scores O ∈ Rk
1: Normalize S according to train data
2: Initiate zeros array O of size k
3: Initiate E ∼ NT ×d (0, I)
4: for i = 1 to k do
5:
Pick x0 = Si
6:
for t = 1 √
to T do √
7:
xt = ᾱt x0 + 1 − ᾱt Et
8:
loss = ||Et − ϵθ (xt , t)||22
9:
Oi += loss
10:
end for
11: end for
12: Return O = {O1 , ..., Ok }

3.2.3

▷ E ∈ RT ×d

▷ Eq. 2

Architecture

Our model ϵθ (xt , t) is a variation of ResNet architecture for tabular data [14] with the utilization of
relevant components from U-Net model used in DDPM [19]. Specifically, we use a time embedding
block defined by the Transformer sinusoidal position embedding [42] and a single residual block
(ResBlock) to combine the feature vectors of the time-step t and the noisy sample xt . The sizes of
the time embedding block and the fully connected (FC) layers are defined as hyperparameters (See
Tab. 4). We use SiLU and Leaky-ReLU with a negative slope of 0.2 as activation functions. Fig. 1
describes the block diagram of our architecture.

Figure 1: Proposed architecture for anomaly detection on tabular data. The model receives noisy
sample xt and time step t that are fed forward to the ResBlock. The output of the ResBlock propagates through the Leaky-ReLU activation function followed by the FC layer to create the noise
estimation of the real noise component in xt .
5

4

Experiments

Datasets. We use 32 anomaly detection datasets from the ADBench repository [16] in this study
(Appx. Tab. 5). Of these, 28 are real-world datasets, and the rest are extracted data-embedding
representations of pre-trained models from the fields of computer vision (CV) and natural language
processing (NLP). Specifically, the CV datasets include FashionMNIST and SVHN, for which both
BERT and RoBERTa versions are utilized, and we randomly select the first class (out of 10 existing)
for testing. The NLP datasets include Amazon and Yelp, and both ViT and ResNet versions are
employed. In addition, due to convergence failure in some of the baselines, we stratified truncate
Census to 50K samples, i.e., we maintain the original anomaly ratio post truncation.

Baseline methods and hyperparameters Settings. We evaluate TabADM against eleven outlier
detectors. Among them, nine are leading detectors from ADBench [16] with a wide variety and two
recent NN based methods. The competitors from ADBench are k Nearest Neighbors (KNN) [33],
Local Outlier Factor (LOF) [8], One-Class Support Vector Machines (OCSVM) [38], PCA-based
Outlier Detector (PCA) [40], Clustring-based Local Outlier Factor (CBLOF) [18], Isolation Forest
(IForest) [29], Copula Based Outlier Detector (COPOD) [26], Histogram-based Outlier Detector
(HBOS) [13] and Empirical Cumulative Distribution-based Outlier Detector (ECOD) [25]. We use
PyOD [47] anomaly detection python package for implementation of baseline methods and use their
default PyOD1 configuration for a fair comparison2 . Additionally, we include GOAD by Bergman
et al. [6] and NeuTraL AD (referred to as NeuTraL) by Qiu et al. [32]. These methods have recently
demonstrated impressive results on tabular data. We adopt the kdd based configuration for both
methods for all experiments. The default hyperparameters we use for the training of TabADM are
summarized in Appx. Tab. 4.

Results We use the Area Under Receiver Operating Characteristic Curve (AUCROC) and Average
Precision (AP) as evaluation metrics. We use a MacBook Pro laptop with M1, 16 GB of memory,
and without GPU for all experimental runs.
In the first part, we follow the ADBench [16] experiment settings and use random stratified sampling to divide the data into 70% for training and 30% for testing. We repeat this process five
times and report the average scores3 . In addition, to evaluate the performance of our method in
high-dimensional data, we sort the 32 datasets in ascending order according to their dimensions and
define the parameter τ as the percentile value corresponding to the dimensions. For each value of τ ,
we partition the datasets into subgroups based on τ , where each subgroup consists of datasets with
dimensions greater than τ . For example, when τ = 10, we form a group comprising the top 90%
datasets with the highest number of variables. We calculate the average AUCROC and AP ranks for
each sub-group for each method. We plot the values of the average ranks of both AUCROC and AP
as a function of τ .
The results for this part are presented in Tab. 1 and 2. The results of our proposed TabADM method
demonstrate that, on average, it outperforms the other baselines in both AUCROC and AP scores,
as well as in average rank, by a significant margin. Additionally, we observe that among the top 10
datasets with the highest dimensionality, TabADM achieves the highest AUCROC (AP) score in 5
(4) datasets. In light of this, we conduct a more in-depth analysis to evaluate the performance of
the methods with respect to the dimensionality of the dataset. As illustrated in Fig. 2, TabADM
demonstrates consistently low average ranks across all percentile values in both AUCROC and AP
scores. Additionally, it can be observed that as the percentile value increases, the performance of
TabADM improves, and the gap to the rest grows. This suggests that our model is particularly
well-suited for large datasets.

1

https://pyod.readthedocs.io/en/latest/pyod.html
For CBLOF, we use nclusters = 9 due to convergence failure in some of the datasets using the default
settings.
3
For CV and NLP datasets, we report the average score of the two different versions.
2

6

Table 1: AUCROC scores for ADBench datasets.
Dataset
Smtp
Mammogra.
Thyroid
Glass
Shuttle
Donors
PageBlocks
Vowels
Pendigits
Hepatitis
Cardio
Cardiotocogr.
Waveform
Letter
Ionosphere
Landsat
Satellite
Satimage-2
Celeba
SpamBase
Campaign
Optdigits
MNIST
Musk
Backdoor
Speech
Census
FashionM.
SVHN
Amazon
Yelp
InternetAds
AVG
AVG Rank

PCA

OCSVM

LOF

CBLOF

HBOS

KNN

COPOD

IF

ECOD

GOAD

NeuTraL

TabADM

81.01
87.05
96.04
74.52
98.95
82.85
90.59
62.54
92.61
79.05
94.27
75.82
62.08
53.31
79.03
36.63
60.34
97.41
78.58
56.36
73.80
49.97
85.35
100.00
88.68
51.39
65.58
86.87
55.57
52.61
56.09
60.24
73.91
6.06

74.94
83.72
87.58
55.27
98.15
73.11
89.30
58.31
92.62
67.86
91.96
79.19
51.86
51.33
74.42
36.62
59.97
97.13
69.70
54.96
66.77
52.84
82.69
81.19
84.77
50.99
52.77
85.67
55.86
52.87
56.30
68.75
69.98
7.88

93.07
73.15
86.37
66.02
53.59
61.65
72.67
94.76
51.44
79.52
64.08
58.43
70.51
87.43
85.93
55.94
55.17
49.86
42.49
45.14
56.57
53.05
68.30
37.80
71.57
53.99
48.05
63.75
65.79
55.39
61.94
64.77
64.01
8.22

83.82
80.15
94.10
85.59
75.59
65.36
87.37
90.69
90.26
73.81
80.27
65.97
71.39
76.28
89.52
62.05
72.97
99.87
60.19
57.78
65.85
73.28
79.67
100.00
83.10
51.17
58.85
88.32
59.70
53.33
60.12
68.41
75.15
5.75

81.48
83.91
95.91
83.23
98.68
71.88
81.14
68.74
92.20
79.76
83.34
60.51
67.84
61.43
65.65
57.01
75.27
97.66
75.63
65.69
79.61
80.34
61.94
100.00
75.66
51.92
62.75
82.94
51.03
53.14
56.44
68.28
74.09
6.16

90.61
82.69
95.49
88.39
63.40
62.68
81.42
98.23
72.97
71.90
72.17
54.63
72.21
87.77
92.43
59.00
65.53
92.92
57.46
54.82
72.40
38.92
81.42
70.93
68.20
52.26
64.12
84.87
62.51
56.32
62.71
70.10
71.92
6.38

90.95
89.00
94.07
78.06
99.41
81.41
87.74
52.45
88.99
82.38
91.16
66.80
71.94
55.03
79.54
42.50
63.47
97.18
75.17
69.11
78.74
66.71
77.92
94.71
78.91
53.13
66.62
84.09
51.50
52.31
55.33
67.51
74.49
6.00

89.19
84.56
97.73
80.65
99.72
77.76
89.26
79.17
94.77
74.29
90.91
68.85
69.17
63.19
84.43
49.49
71.38
99.21
69.91
61.37
71.12
69.23
80.47
99.97
73.22
53.50
60.77
85.44
55.97
52.92
56.93
67.90
75.70
5.34

87.30
88.96
97.84
76.02
99.27
88.80
91.55
61.43
91.60
75.48
92.90
79.13
59.38
56.82
73.66
37.09
58.51
96.14
75.81
66.00
77.51
58.93
75.10
95.58
84.56
51.46
65.41
85.23
53.09
52.32
55.60
67.57
74.25
6.13

87.61
57.27
73.19
64.41
98.56
30.90
80.24
93.46
63.17
69.05
59.40
39.79
70.80
80.55
87.49
59.11
64.90
97.47
26.69
38.26
41.93
69.49
83.58
100.00
90.48
46.74
53.62
72.89
58.93
50.36
50.36
41.82
65.70
8.31

79.15
52.89
92.20
49.25
95.11
50.26
83.98
45.94
86.04
55.75
26.30
77.29
66.16
23.06
83.41
64.16
77.25
99.94
70.79
40.02
74.77
58.32
88.49
76.15
89.99
40.19
50.03
86.59
64.10
54.57
59.58
46.10
65.87
7.53

86.21
82.79
93.84
88.71
98.97
72.48
90.83
96.80
86.20
71.43
81.70
60.97
71.40
91.04
92.67
58.61
72.53
99.31
69.05
58.93
72.36
59.23
86.13
100.00
91.83
54.54
64.99
89.43
62.75
54.71
58.77
76.48
77.99
4.25

Table 2: AP scores for ADBench datasets.
Dataset
Smtp
Mammogra.
Thyroid
Glass
Shuttle
Donors
PageBlocks
Vowels
Pendigits
Hepatitis
Cardio
Cardiotocogr.
Waveform
Letter
Ionosphere
Landsat
Satellite
Satimage-2
Celeba
SpamBase
Campaign
Optdigits
MNIST
Musk
Backdoor
Speech
Census
FashionM.
SVHN
Amazon
Yelp
InternetAds
AVG
AVG Rank

PCA

OCSVM

LOF

CBLOF

HBOS

KNN

COPOD

IF

ECOD

GOAD

NeuTraL

TabADM

44.75
19.93
42.26
20.65
90.35
16.68
52.82
8.54
19.86
50.94
57.18
48.48
4.80
10.16
73.13
16.43
60.80
87.19
11.28
42.21
28.97
2.67
39.63
100.00
52.13
2.98
8.76
26.42
7.03
5.43
6.01
27.54
33.94
5.72

5.35
11.84
19.49
16.81
94.69
10.42
51.88
9.19
19.51
28.67
53.33
54.43
3.94
8.62
72.76
16.33
58.98
86.03
7.41
41.59
28.58
2.90
32.97
10.47
8.62
3.07
6.27
24.90
7.02
5.44
6.11
53.29
26.90
7.59

1.43
10.33
16.87
22.53
9.68
10.50
35.77
43.12
3.88
37.08
18.25
30.42
9.22
43.76
81.22
26.49
38.45
3.37
1.78
35.91
12.12
5.00
22.73
3.57
20.69
3.90
5.51
11.27
9.83
5.69
7.56
37.81
19.55
7.78

44.42
7.96
26.16
25.28
40.51
8.25
56.64
23.99
16.37
31.76
36.56
44.03
15.11
19.55
87.74
29.36
62.19
97.32
3.30
43.27
21.12
4.99
30.91
100.00
7.19
3.08
6.94
28.40
8.12
5.48
6.50
53.15
31.11
5.78

8.70
19.35
52.55
25.42
95.07
12.24
35.34
11.64
24.16
43.31
42.68
39.37
5.08
10.32
46.00
23.26
67.70
80.89
9.77
50.36
37.45
11.01
13.41
100.00
5.47
3.19
7.88
23.65
6.00
5.48
6.15
52.33
30.48
5.94

48.38
16.10
29.88
26.82
16.42
10.64
45.44
67.26
6.23
29.73
31.25
33.88
10.77
34.73
92.36
25.95
50.55
34.83
2.69
42.70
26.43
2.54
38.31
10.78
30.55
3.37
8.16
23.96
8.81
5.79
7.57
41.63
27.02
6.34

0.46
40.23
19.41
18.97
95.56
20.76
37.42
4.05
16.54
50.84
52.56
40.78
5.67
7.36
68.86
17.89
57.18
79.94
9.63
54.74
37.01
4.21
22.37
35.61
6.92
3.03
8.98
24.21
6.16
5.40
6.04
50.35
28.41
6.94

0.44
19.50
60.16
21.59
97.92
13.07
45.78
15.79
28.71
34.82
48.48
43.03
6.21
9.91
79.49
20.12
66.20
91.31
7.08
50.14
30.23
5.19
28.43
99.80
3.91
2.84
7.33
23.04
7.04
5.49
6.17
50.04
32.16
5.91

62.09
40.67
49.60
25.68
89.81
26.33
52.34
10.46
25.19
36.97
53.55
51.65
4.47
8.63
65.74
16.61
52.65
68.14
9.86
52.21
35.57
3.40
18.27
48.73
9.25
3.28
8.58
24.84
6.31
5.35
5.84
50.46
31.95
5.91

41.92
5.84
35.65
16.67
85.60
5.13
46.15
39.49
5.92
33.23
29.43
24.40
44.14
30.47
85.59
24.89
43.94
43.72
1.39
34.74
10.46
5.01
38.35
100.00
55.48
2.38
6.54
17.74
7.84
5.26
5.01
53.05
29.70
8.06

31.36
3.04
34.44
7.80
52.89
5.58
29.95
5.28
14.95
22.54
7.95
61.81
5.32
4.25
75.54
26.72
72.87
96.35
4.62
35.71
31.10
3.70
41.96
6.98
36.38
1.48
5.78
31.89
8.94
5.63
6.85
17.89
24.92
7.41

46.10
15.68
31.19
39.06
92.52
11.59
57.27
64.12
9.83
32.30
31.77
37.07
6.26
49.66
92.37
24.95
56.15
62.75
4.37
43.69
29.52
4.22
44.17
100.00
41.29
3.99
8.51
37.82
8.49
5.60
6.55
53.05
36.00
4.63

7

Figure 2: Average AUCROC (top) and AP (bottom) rank per method as a function of τ , where τ
is the percentile value corresponding to the number of dimensions. For example, when τ = 10,
we form a subgroup comprising the top 90% of datasets with the highest number of variables and
present the average ranks on this subgroup. We limit τ to a maximum of 70 to avoid an evaluation
on a small subset of datasets.
In the second part, we randomly divide Satellite, Cardiotocography, and SpamBase datasets into
training and test sets using a 70-30 split. Then, we create 11 sub-training sets with varying contamination ratios from 0% to 10%. In addition, we randomly fix a 10% contamination ratio in the test
set. We repeat this process 5 times and plot the average AUCROC and AP scores as a function of
contamination ratios for each dataset.
The results in this part are shown in Fig. 3. As the level of contamination in the training set increases,
there is a decline in both the AUCROC and AP scores. This can be attributed to the fact that the
model learns the anomalous samples in addition to the inlier samples. As a result, the ability of
our model to accurately distinguish between inliers and outliers is hindered, leading to a decrease in
performance.

Figure 3: AUCROC (left) and AP (right) scores for the Satellite, Cardiotocography, and SpamBase
datasets decrease as the contamination percentage increases. This is due to the increasing influence
of anomalous samples on the overall probability distribution learned by the model.
8

In the third part, we investigate the impact of different training hyperparameters on the performance
of our model. We examine the relationship between the AUCROC and AP scores and the number
of training iterations for Landsat, Letter, and Musk. In addition, we investigate the influence of the
number of rejections samples (m) on the performance. As in previous parts, we use a 70-30 train-test
random split over five times and report the average AUCROC and AP scores for m = 0, 1, 4, 7.
Fig. 4 and Tab. 3 present the results for this part. As shown in Fig. 4, as the number of training
steps increases, the performance of all datasets improves. However, the improvement rate varies
among different datasets. Tab. 3 demonstrates that excluding the sample with the highest loss in a
batch during training (m = 1) leads to the highest average scores. This indicates that the model is
more robust to anomalies, resulting in better modeling of the normal underlying distribution and,
consequently, improved overall performance.

Figure 4: AUCROC (left) and AP (right) scores for the Landsat, Letter, and Mask datasets as functions of training steps.
Table 3: Comparison of AUCROC (left) and AP (right) scores for the Landset, Letter and Musk
datasets for different values of rejection samples m from batch of size 8.
AUCROC (%)

5

AP (%)

Dataset

m=0

m=1

m=4

m=7

m=0

m=1

m=4

m=7

Landsat
Letter
Musk

56.78
91.28
99.10

58.61
91.04
100.00

55.78
90.75
100.00

54.88
81.06
100.00

24.41
48.24
72.56

24.95
49.66
100.00

22.73
39.81
100.00

22.67
22.00
100.00

Conclusion and Future Work

In this paper, we introduce a novel unsupervised outlier detection method, TabADM, which utilizes
the diffusion models technique to estimate the probability distribution of the data. It then assigns
outlier scores to unseen samples based on their probability of being generated from the model. In
addition, a rejection scheme is introduced to enhance performance when outliers are present in the
data. TabADM exhibits strong training stability and alleviates the need for hyperparameter tuning. Furthermore, it demonstrates exceptional performance in high-dimensional datasets, surpassing
other SOTA methods.
TabADM has certain drawbacks, including long training and inference times compared to other
methods and a lack of interpretability. Future work could focus on improving these drawbacks.
For example, the inference time can be reduced by decreasing the number of diffusion steps used
per sample, although this may impact performance. Additionally, efforts could be made to enhance
interpretability. This could be achieved through simple measures such as identifying which features
contribute most significantly to the total loss, as well as more complex measures such as identifying
common feature patterns in the data that may serve as indicators for abnormality. Another possible
future research direction would be to extend the capabilities of TabADM such as enabling it to
handle missing feature values.
9

Acknowledgments
Funding: This research was partially supported by the Israel Science Foundation (ISF, 1556/17,
1873/21), Israel Ministry of Science Technology and Space 3-16414, 3-14481,3-17927) and Magneton Playtika.4758/2

References
[1] C. C. Aggarwal, A. Hinneburg, and D. A. Keim. On the Surprising Behavior of Distance
Metrics in High Dimensional Space. In Database Theory — ICDT 2001, pages 420–434,
2001.
[2] C. C. Aggarwal and P. S. Yu. Outlier Detection for High Dimensional Data. In Proceedings of
the 2001 ACM SIGMOD international conference on Management of data, pages 37–46, 2001.
[3] J. An and S. Cho. Variational Autoencoder based Anomaly Detection using Reconstruction
Probability. Special lecture on IE, 2(1):1–18, 2015.
[4] Y. Balaji, S. Nah, X. Huang, A. Vahdat, J. Song, Q. Zhang, K. Kreis, M. Aittala, T. Aila,
S. Laine, B. Catanzaro, T. Karras, and M.-Y. Liu. eDiff-I: Text-to-Image Diffusion Models
with an Ensemble of Expert Denoisers, 2023. arXiv:2211.01324.
[5] D. Baranchuk, I. Rubachev, A. Voynov, V. Khrulkov, and A. Babenko. Label-Efficient Semantic Segmentation With Diffusion Models. In ICLR, 2022.
[6] L. Bergman and Y. Hoshen. Classification-Based Anomaly Detection for General Data. In
ICLR, 2020.
[7] Y Bregman, O Lindenbaum, and N Rabin. Array based earthquakes-explosion discrimination
using diffusion maps. Pure and Applied Geophysics, 178:2403–2418, 2021.
[8] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander. LOF: Identifying Density-Based Local
Outliers. SIGMOD Rec., 29(2):93–104, 2000.
[9] B. Cao, M. Mao, S. Viidu, and P. Yu. Collective Fraud Detection Capturing Inter-Transaction
Dependency. In Proceedings of the KDD 2017: Workshop on Anomaly Detection in Finance,
volume 71, pages 66–75, 2018.
[10] N. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and W. Chan. WaveGrad: Estimating
Gradients for Waveform Generation. In ICLR, 2021.
[11] L. Deecke, R. Vandermeulen, L. Ruff, S. Mandt, and M. Kloft. Anomaly Detection with
Generative Adversarial Networks, 2018.
[12] P. Dhariwal and A. Nichol. Diffusion Models Beat GANs on Image Synthesis. In NeurIPS,
volume 34, pages 8780–8794, 2021.
[13] M. Goldstein and A. Dengel. Histogram-Based Outlier Score (HBOS): A fast Unsupervised
Anomaly Detection Algorithm. KI: Poster and Demo Track, 09 2012.
[14] Y. Gorishniy, I. Rubachev, V. Khrulkov, and A. Babenko. Revisiting Deep Learning Models
for Tabular Data. In NeurIPS, 2021.
[15] F. E. Grubbs. Procedures for Detecting Outlying Observations in Samples. Technometrics,
11(1):1–21, 1969.
[16] S. Han, X. Hu, H. Huang, M. Jiang, and Y. Zhao. ADBench: Anomaly Detection Benchmark.
In NeurIPS, 2022.
[17] Z. He, S. Deng, and X. Xu. A Unified Subspace Outlier Ensemble Framework for Outlier
Detection. In Advances in Web-Age Information Management, pages 632–637, 2005.
[18] Z. He, X. Xu, and S. Deng. Discovering Cluster-Based Local Outliers. Pattern Recognition
Letters, 24(9):1641–1650, 2003.
10

[19] J. Ho, A. Jain, and P. Abbeel. Denoising Diffusion Probabilistic Models. In NeurIPS, volume 33, pages 6840–6851, 2020.
[20] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet. Video Diffusion
Models. In NeurIPS, volume 35, pages 8633–8646, 2022.
[21] J. Hollmén and V. Tresp. Call-Based Fraud Detection in Mobile Communication Networks
Using a Hierarchical Regime-Switching Model. In NIPS, volume 11, 1998.
[22] Lina Irshaid, Jonathan Bleiberg, Ethan Weinberger, James Garritano, Rory M Shallis, Jonathan
Patsenker, Ofir Lindenbaum, Yuval Kluger, Samuel G Katz, and Mina L Xu. Histopathologic
and machine deep learning criteria to predict lymphoma transformation in bone marrow biopsies. Archives of Pathology & Laboratory Medicine, 146(2):182–193, 2022.
[23] J. Laurikkala and M. Juhola and E. Kentala. Informal Identification of Outliers in Medical Data. In Workshop Notes of the 14th European Conference on Artificial Intelligence
(ECAI-2000): The Fifth Workshop on Intelligent Data Analysis in Medicine and Pharmacology
(IDAMAP-2000), pages 20–24, 2000.
[24] C. Kruegel, D. Mutz, W. Robertson, and F. Valeur. Bayesian event classification for intrusion
detection. In 19th Annual Computer Security Applications Conference, pages 14–23, 2003.
[25] Z. Li, X. Hu Y. Zhao, N. Botta, C. Ionescu, and G. H. Chen. ECOD: Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions. IEEE Transactions on Knowledge
and Data Engineering (TKDE), pages 1–1, 2022.
[26] Z. Li, Y. Zhao, N. Botta, C. Ionescu, and X. Hu. COPOD: Copula-Based Outlier Detection. In
2020 IEEE International Conference on Data Mining (ICDM), pages 1118–1123, 2020.
[27] Ofir Lindenbaum, Yariv Aizenbud, and Yuval Kluger. Probabilistic robust autoencoders for
outlier detection. arXiv preprint arXiv:2110.00494, 2021.
[28] Ofir Lindenbaum, Neta Rabin, Yuri Bregman, and Amir Averbuch. Multi-channel fusion for
seismic event detection and classification. In 2016 IEEE International Conference on the
Science of Electrical Engineering (ICSEE), pages 1–5. IEEE, 2016.
[29] F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation Forest. In 2008 IEEE International Conference
on Data Mining (ICDM), pages 413–422, 2008.
[30] Y. Liu, Z. Li, C. Zhou, Y. Jiang, J. Sun, M. Wang, and X. He. Generative adversarial active
learning for unsupervised outlier detection. TKDE, 32(8):1517–1528, 2019.
[31] C. Qiu, A. Li, M. Kloft, M. Rudolph, and S. Mandt. Latent Outlier Exposure for Anomaly
Detection with Contaminated Data. In ICML, pages 18153–18167, 2022.
[32] C. Qiu, T. Pfrommer, M. Kloft, S. Mandt, and M. Rudolph. Neural Transformation Learning
for Deep Anomaly Detection Beyond Images. In ICML, pages 8703–8714, 2021.
[33] S. Ramaswamy, R. Rastogi, and K. Shim. Efficient algorithms for mining outliers from large
data sets. SIGMOD Rec., 2000.
[34] Amit Rozner, Barak Battash, Henry Li, Lior Wolf, and Ofir Lindenbaum. Anomaly detection
with variance stabilized density estimation. arXiv preprint arXiv:2306.00582, 2023.
[35] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi. Photorealistic
Text-to-Image Diffusion Models with Deep Language Understanding. In NeurIPS, volume 35,
pages 36479–36494, 2022.
[36] D. Savage, X. Zhang, X. Yu, P. Chou, and Q. Wang. Anomaly Detection in Online Social
Networks. Social Networks, 39:62–70, 2014.
[37] T. Schlegl, P. Seeböck, S. M Waldstein, U. Schmidt-Erfurth, and G. Langs. Unsupervised
anomaly detection with generative adversarial networks to guide marker discovery. In Information Processing in Medical Imaging: 25th International Conference, pages 146–157, 2017.
11

[38] B. Schölkopf, R. C. Williamson, A. Smola, J. Shawe-Tailor, and J. Platt. Support Vector
Method for Novelty Detection. In NIPS, volume 12, 1999.
[39] K. Sequeira and M. Zaki. ADMIT: Anomaly-Based Data Mining for Intrusions. In Proceedings
of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pages 386–395, 2002.
[40] M.-L. Shyu, S.-C. Chen, K. Sarinnapakorn, and L. Chang. A Novel Anomaly Detection
Scheme Based on Principal Component Classifier. Technical report, Miami Univ Coral Gables
Fl Dept of Electrical and Computer Engineering, 2003.
[41] A. O. Tur, N. Dall’Asen, C. Beyan, and E. Ricci. Exploring Diffusion Models for Unsupervised
Video Anomaly Detection, 2023. arXiv:2304.05841.
[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention Is All You Need. In NIPS, volume 30, 2017.
[43] J. Wolleb, F. Bieder, R. Sandkühler, and P. C. Cattin. Diffusion Models for Medical Anomaly
Detection. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2022:
25th International Conference, pages 35–45, 2022.
[44] J. Wolleb, R. Sandkühler, F. Bieder, P. Valmaggia, and P. C. Cattin. Diffusion Models for
Implicit Image Segmentation Ensembles. In International Conference on Medical Imaging
with Deep Learning, pages 1336–1348, 2022.
[45] J. Wyatt, A. Leach, S. M. Schmon, and C. G. Willcocks. AnoDDPM: Anomaly Detection
with Denoising Diffusion Probabilistic Models using Simplex Noise. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 649–655,
2022.
[46] X. Yang, L. Latecki, and D. Pokrajac. Outlier Detection with Globally Optimal ExemplarBased GMM. SIAM, pages 145–154, 2009.
[47] Y. Zhao, Z. Nasrullah, and Z. Li. PyOD: A Python Toolbox for Scalable Outlier Detection.
JMLR, 20(96):1–7, 2019.
[48] C. Zhou and R. C. Paffenroth. Anomaly Detection with Robust Deep Autoencoders. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pages 665–674, 2017.
[49] B. Zong, Q. Song, M. R. Min, W. Cheng, C. Lumezanu, D. Cho, and H. Chen. Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection. In ICLR, 2018.

12

Appendix
A

Defualt hyperparameters

Tab. 4 presents the default hyperparameters employed for the training procedure in our experiments.
While most hyperparameters remain constant, the learning rate and the size of the fully connected
(FC) layers are determined by the number of dimensions (d) of the training dataset.
Table 4: Default hyperparameters for TabADM training procedure.
# Dims. in dataset (d)

d ≤ 100

100 < d ≤ 1000

1000 < d ≤ 2000

FC layers size
Learning rate
Weight decay
Time embedding size
Batch size (k)
Rejection samples (m)
Train steps (e)
Noise schedule
Diffusion timesteps (T)
Loss type

512
1X10−3
1X10−4
64
8
1
50K
linear
100
Simplified MSE of ϵ

1024
2X10−4
1X10−4
64
8
1
50K
linear
100
Simplified MSE of ϵ

2048
2X10−4
1X10−4
64
8
1
50K
linear
100
Simplified MSE of ϵ

13

B

Evaluation datasets

Tab. 5 lists the ADBench [16] datasets used for evaluation. To ensure a fair comparison between the
baselines, we selected diverse datasets in terms of their dimensions (d), number of samples (n), and
anomaly rates.
The list contains 32 datasets. Among them, 28 real-world tabular datasets and the other 4 are extracted feature embedding representations of pre-trained models from CV and NLP fields. The
CV datasets include FashionMNIST and SVHN, and we randomly selected the first class (out of 10
classes) for testing. In addition, each dataset has two versions: the features of the first version are
based on a pre-trained ResNet model with 512 dimensions, while the features of the second version
are based on a pre-trained ViT model with 1000 dimensions. Similarly, the NLP datasets include
Amazon and Yelp, each with two versions. One is based on a pre-trained BERT model, and the other
is based on a pre-trained RoBERTa model. Both versions have 768 dimensions. For the CV and NLP
datasets, we reported the average AUCROC and AP scores across the two versions. Lastly, due to
convergence failure in some of the baselines, we randomly sub-sampled Census to 50,000 samples
in a stratified way to preserve the original anomaly rate post-truncation.
Table 5: List of ADBench datasets used for evaluation.
Dataset

# Dims. (d)

# Samp. (n)

Anomaly rate (%)

Smtp
Mammography
Thyroid
Glass
Shuttle
Donors
PageBlocks
Vowels
Pendigits
Hepatitis
Cardio
Cardiotocography
Waveform
Letter
Ionosphere
Landsat
Satellite
Satimage-2
Celeba
SpamBase
Campaign
Optdigits
MNIST
Musk
Backdoor
Speech
Census
FashionMNIST
SVHN
Amazon
Yelp
InternetAds

3
6
6
7
9
10
10
12
16
19
21
21
21
32
33
36
36
36
39
57
62
64
100
166
196
400
500
512/1000
512/1000
768/768
768/768
1555

95156
11183
3772
214
49097
619326
5393
1456
6870
80
1831
2114
3443
1600
351
6435
6435
5803
202599
4207
41188
5216
7603
3062
95329
3686
50000
6315
5208
10000
5000
1966

0.03
2.32
2.47
4.21
7.15
5.93
9.46
3.43
2.27
16.25
9.61
22.04
2.90
6.25
35.90
20.71
31.64
1.22
2.24
39.91
11.27
2.88
9.21
3.17
2.44
1.65
6.20
5.00
5.00
5.00
5.00
18.72

14

